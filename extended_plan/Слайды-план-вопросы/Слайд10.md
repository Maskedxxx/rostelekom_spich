# **ЧАСТЬ 2: КАРТА LLM МИРА И РЕАЛИИ**

## **Слайд 10: Обзор Игроков и Состояние Рынка**

* **Заголовок:** Карта LLM мира: кто есть кто?  
* **Тезис (из мыслей):** Введение в список игроков их состояние (облако производителей, типов).  
* **Текст:** Основные игроки делятся на три лагеря: Облачные гиганты (закрытые модели), Open Source сообщество и Региональные провайдеры.  
* **Визуализация:** Логотипы и классификация (Closed vs. Open).

## **Слайд 11: Размеры Моделей: Градация**

* **Заголовок:** Параметры и Возможности: от 1B до 1T.  
* **Тезис (из мыслей):** Таблица или график по размерам (от 1b до 1t).  
* **Текст:** Сравнение Large Language Models (LLM, 70B+), Small Language Models (SLM, 1B-20B) и Mixture of Experts (MoE). Показать, что размер больше не единственный фактор успеха.  
* **Визуализация:** Таблица с колонками: Размер, Примеры, Типичное Использование.

## **Слайд 12: Вариант Инференса: Облако Вендоров**

* **Заголовок:** Точка Доступа I: Облако от Вендоров.  
* **Тезис (из мыслей):** Варианты инференса: облако от вендоров.  
* **Текст:** Самый простой способ начать: доступ через API. Преимущества: не нужно HW, готовая инфраструктура, максимальные модели. Недостатки: стоимость, зависимость, комплаенс.  
* **Визуализация:** Схема: Пользователь $\\to$ API $\\to$ Модель в Облаке.

## **Слайд 13: Вариант Инференса: Агрегаторы / РФ Провайдеры**

* **Заголовок:** Точка Доступа II: Агрегаторы и Региональные Провайдеры.  
* **Тезис (из мыслей):** Агрегаторы (OpenRouter) и Российские провайдеры (Yandex, Sber).  
* **Текст:** Агрегаторы для гибкости (выбор модели), региональные провайдеры для комплаенса и работы с национальными языковыми особенностями.  
* **Визуализация:** Логотипы OpenRouter, Yandex, Sber.

## **Слайд 14: Вариант Инференса: Локальный Запуск (On-Premise)**

* **Заголовок:** Точка Доступа III: Локальный Запуск (On-Premise).  
* **Тезис (из мыслей):** Локальный запуск – ключ к контролю TCO и данных.  
* **Текст:** Главные преимущества: 100% контроль над данными и безопасностью; снижение операционных расходов (TCO) при высокой нагрузке; возможность использовать SLM.  
* **Визуализация:**

## **Слайд 15: SLM vs LLM: Экономика и Фокус**

* **Заголовок:** Стратегия выбора: Когда нужен SLM?  
* **Тезис (из мыслей):** SLM: Низкий TCO, on-prem. LLM: Высокая точность, облако.  
* **Текст:** Не всегда нужен самый большой и дорогой LLM. Для большинства задач SDLC (ревью, генерация тестов) достаточно быстрого и дешевого SLM.  
* **Визуализация:** Сравнение: Дорогой LLM (Облако) vs Дешевый SLM (On-prem).

## **Слайд 16: SLM как Идеальный Инструмент**

* **Заголовок:** SLM: Рабочая лошадка для SDLC.  
* **Тезис (из мыслей):** SLM — идеальный "рабочий" инструмент.  
* **Текст:** SLM отлично подходит для задач с четкой структурой, где вывод легко контролировать (например, JSON-вывод, классификация). Они отлично интегрируются в пайплайны.

## **Слайд 17: Deep Dive: On-Premise (Инфраструктура)**

* **Заголовок:** On-Premise: Краткий Ликбез по Инфраструктуре.  
* **Тезис (из мыслей, объединенный):** Стек: vLLM, TGI, LMStudio, llama.cpp. Квантование (GGUF/GGML) — почему это уменьшает стоимость HW.  
* **Текст:** Для эффективной работы на собственном железе нужны специализированные фреймворки. **vLLM/TGI** для высокой пропускной способности. **llama.cpp/LMStudio** для экспериментов и десктопа.  
* **Визуализация:** Схема, показывающая, как vLLM с PagedAttention управляет VRAM.

## **Слайд 18: On-Premise (Квантование и TCO)**

* **Заголовок:** GGUF, VRAM и TCO.  
* **Тезис (из мыслей, объединенный):** Рекомендации по GPU, VRAM и GGUF. Оценка стоимости "домашней лаборатории" (для прототипов).  
* **Текст:** **Квантование (GGUF, AWQ, FP8):** сжимает модель в 4-8 раз, позволяя запускать большие модели на дешевых GPU. Это напрямую снижает **TCO**.  
* **Визуализация:** График: Объем VRAM, необходимый для 7B/13B/70B модели (FP16 vs GGUF).

## **Слайд 19: Deep Dive: LMStudio / llama.cpp**

* **Заголовок:** Инструменты для прототипов и экспериментов.  
* **Тезис (из мыслей):** LMStudio / llama.cpp.  
* **Текст:** Простые инструменты для запуска локальных моделей (GGUF) на CPU/GPU, идеально для быстрого тестирования идей.

## **Слайд 20: Hardware: Выбор GPU и VRAM**

* **Заголовок:** Выбор оборудования и VRAM.  
* **Тезис (из мыслей):** Рекомендации по GPU, VRAM и GGUF.  
* **Текст:** Краткие советы, на что обращать внимание при выборе GPU для on-prem (объем VRAM важнее частоты/ядер).